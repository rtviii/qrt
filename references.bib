@ARTICLE{2016aravkinlevel,
  author =       {{Aravkin}, A.~Y. and {Burke}, J.~V. and
                  {Drusvyatskiy}, D. and {Friedlander}, M.~P. and
                  {Roy}, S.},
  title =        "Level-set methods for convex optimization",
  journal =      "Mathematical Programming",
  year =         2018,
  volume =       174,
  number =       {1-2},
  pages =        {359--390},
  doi =          {10.1007/s10107-018-1351-8}
}
@article{beck_SmoothingFirstOrder_2012,
  title = {Smoothing and {{First Order Methods}}: {{A Unified Framework}}},
  shorttitle = {Smoothing and {{First Order Methods}}},
  author = {Beck, Amir and Teboulle, Marc},
  date = {2012},
  journaltitle = {SIAM Journal on Optimization},
  volume = {22},
  number = {2},
  pages = {557--580},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {{Philadelphia, United States}},
  issn = {10526234},
  doi = {10.1137/100818327},
  url = {https://www.proquest.com/docview/1037526940/abstract/93E9A3AFE141473FPQ/1},
  urldate = {2023-06-22},
  abstract = {We propose a unifying framework that combines smoothing approximation with fast first order algorithms for solving nonsmooth convex minimization problems. We prove that independently of the structure of the convex nonsmooth function involved, and of the given fast first order iterative scheme, it is always possible to improve the complexity rate and reach an \$O(\textbackslash varepsilon\^\{-1\})\$ efficiency estimate by solving an adequately smoothed approximation counterpart. Our approach relies on the combination of the notion of smoothable functions that we introduce with a natural extension of the Moreau-infimal convolution technique along with its connection to the smoothing mechanism via asymptotic functions. This allows for clarification and unification of several issues on the design, analysis, and potential applications of smoothing methods when combined with fast first order algorithms. [PUBLICATION ABSTRACT]},
  langid = {english},
  pagetotal = {24},
  keywords = {Algorithms,Approximation,Convex analysis,Iterative methods,Mathematics,Methods},
  file = {/Users/mpf/Documents/Zotero/storage/LTSG8B8A/Beck and Teboulle - 2012 - Smoothing and First Order Methods A Unified Frame.pdf}
}
@article{lewis_ConvexAnalysisHermitian_1996,
  title = {Convex {{Analysis}} on the {{Hermitian Matrices}}},
  author = {Lewis, A. S.},
  date = {1996-02},
  journaltitle = {SIAM Journal on Optimization},
  shortjournal = {SIAM J. Optim.},
  volume = {6},
  number = {1},
  pages = {164--177},
  issn = {1052-6234, 1095-7189},
  doi = {10.1137/0806009},
  url = {http://epubs.siam.org/doi/10.1137/0806009},
  urldate = {2023-04-07},
  langid = {english},
  keywords = {operator functions}
}
@article{donoho2003does,
  title={When does non-negative matrix factorization give a correct decomposition into parts?},
  author={Donoho, David and Stodden, Victoria},
  journal={Advances in neural information processing systems},
  volume={16},
  year={2003}
}
@book{bermanCompletelyPositiveMatrices2003,
  title = {Completely Positive Matrices},
  author = {Berman, Abraham and Shaked-Monderer, Naomi},
  date = {2003},
  publisher = {World Scientific},
  url = {https://books.google.com/books?hl=en&lr=&id=n6fUCgAAQBAJ&oi=fnd&pg=PR7&dq=A.+Bermanand+and+N.+Shaked-Monderer,+Completely+positive+matrices,+World+Scientific,+2003.&ots=xCDMpb0LtS&sig=iG7ZEVLGb7zWN-MrtLJrTKQJeco},
  urldate = {2024-03-04}
}

@article{burerOptimizingPolyhedralsemidefiniteRelaxation2010,
  title = {Optimizing a Polyhedral-Semidefinite Relaxation of Completely Positive Programs},
  author = {Burer, Samuel},
  date = {2010-03},
  journaltitle = {Mathematical Programming Computation},
  shortjournal = {Math. Prog. Comp.},
  volume = {2},
  number = {1},
  pages = {1--19},
  issn = {1867-2949, 1867-2957},
  doi = {10.1007/s12532-010-0010-8},
  url = {http://link.springer.com/10.1007/s12532-010-0010-8},
  urldate = {2024-03-04},
  langid = {english},
  file = {/Users/mpf/Documents/Zotero/storage/WUASQWYQ/Burer - 2010 - Optimizing a polyhedral-semidefinite relaxation of.pdf}
}

@online{IsometryConvexityDimensionality,
  title = {Isometry and Convexity in Dimensionality Reduction - {{ProQuest}}},
  url = {https://www.proquest.com/openview/3256a7d5212a0cf21fa290a0bc578e2c/1?pq-origsite=gscholar&cbl=18750},
  urldate = {2024-03-04},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english},
  file = {/Users/mpf/Documents/Zotero/storage/A7MZ2US4/1.html}
}

@article{leeAlgorithmsNonnegativeMatrix2000,
  title = {Algorithms for Non-Negative Matrix Factorization},
  author = {Lee, Daniel and Seung, H. Sebastian},
  date = {2000},
  journaltitle = {Advances in neural information processing systems},
  volume = {13},
  url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html},
  urldate = {2024-03-04},
  file = {/Users/mpf/Documents/Zotero/storage/DNAJ99CC/Lee and Seung - 2000 - Algorithms for non-negative matrix factorization.pdf}
}

@article{leeLearningPartsObjects1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  date = {1999-10},
  journaltitle = {Nature},
  volume = {401},
  number = {6755},
  pages = {788--791},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/44565},
  url = {https://www.nature.com/articles/44565},
  urldate = {2024-03-04},
  abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@article{linProjectedGradientMethods2007,
  title = {Projected {{Gradient Methods}} for {{Nonnegative Matrix Factorization}}},
  author = {Lin, Chih-Jen},
  date = {2007-10},
  journaltitle = {Neural Computation},
  volume = {19},
  number = {10},
  pages = {2756--2779},
  issn = {0899-7667},
  doi = {10.1162/neco.2007.19.10.2756},
  url = {https://ieeexplore.ieee.org/abstract/document/6795860},
  urldate = {2024-03-04},
  abstract = {Nonnegative matrix factorization (NMF) can be formulated as a minimization problem with bound constraints. Although bound-constrained optimization has been studied extensively in both theory and practice, so far no study has formally applied its techniques to NMF. In this letter, we propose two projected gradient methods for NMF, both of which exhibit strong optimization properties. We discuss efficient implementations and demonstrate that one of the proposed methods converges faster than the popular multiplicative update approach. A simple Matlab code is also provided.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/mpf/Documents/Zotero/storage/4Q7ISTW2/Lin - 2007 - Projected Gradient Methods for Nonnegative Matrix .pdf;/Users/mpf/Documents/Zotero/storage/QC5GHX77/6795860.html}
}

@article{paateroPositiveMatrixFactorization1994,
  title = {Positive Matrix Factorization: {{A}} Non-Negative Factor Model with Optimal Utilization of Error Estimates of Data Values},
  shorttitle = {Positive Matrix Factorization},
  author = {Paatero, Pentti and Tapper, Unto},
  date = {1994},
  journaltitle = {Environmetrics},
  volume = {5},
  number = {2},
  pages = {111--126},
  issn = {1099-095X},
  doi = {10.1002/env.3170050203},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
  urldate = {2024-03-04},
  abstract = {A new variant ‘PMF’ of factor analysis is described. It is assumed that X is a matrix of observed data and σ is the known matrix of standard deviations of elements of X. Both X and σ are of dimensions n × m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n × p, F is the unknown right hand factor matrix (loadings) of dimensions p × m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by σ is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
  langid = {english},
  keywords = {Alternating regression,Error estimates,Factor analysis,Principal component analysis,Repetitive measurements,Scaling,Weighted least squares},
  file = {/Users/mpf/Documents/Zotero/storage/UKPX8VXA/env.html}
}
@book{hastieElementsStatisticalLearning2001,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Friedman, Jerome and Tibshirani, Robert},
  date = {2001},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-0-387-21606-5},
  url = {http://link.springer.com/10.1007/978-0-387-21606-5},
  urldate = {2024-03-04},
  isbn = {978-1-4899-0519-2 978-0-387-21606-5},
  keywords = {algorithms,bioinformatics,Boosting,classification,clustering,data mining,ensemble method,learning,machine learning,neural networks,Random Forest,statistics,supervised learning,Support Vector Machine,unsupervised learning}
}
@article{bertsekasNonlinearProgramming1997,
  title = {Nonlinear {{Programming}}},
  author = {Bertsekas, D. P.},
  date = {1997-03-01},
  journaltitle = {Journal of the Operational Research Society},
  publisher = {Taylor \& Francis},
  doi = {10.1057/palgrave.jors.2600425},
  url = {https://www.tandfonline.com/doi/abs/10.1057/palgrave.jors.2600425},
  urldate = {2024-03-04},
  abstract = {(1997). Nonlinear Programming. Journal of the Operational Research Society: Vol. 48, No. 3, pp. 334-334.},
  langid = {english},
  file = {/Users/mpf/Documents/Zotero/storage/K8UZJ48T/palgrave.jors.html}
}
@article{kolda_TensorDecompositionsApplications_2009,
  title = {Tensor {{Decompositions}} and {{Applications}}},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  date = {2009-08-06},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {51},
  number = {3},
  pages = {455--500},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/07070111X},
  url = {http://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2022-11-01},
  abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
  langid = {english},
  file = {/Users/mpf/Documents/Zotero/storage/5HV3NICR/Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf}
}
@book{vasiloglouiiIsometryConvexityDimensionality2009,
  title = {Isometry and Convexity in Dimensionality Reduction},
  author = {Vasiloglou II, Nikolaos},
  date = {2009},
  publisher = {Georgia Institute of Technology},
  url = {https://search.proquest.com/openview/3256a7d5212a0cf21fa290a0bc578e2c/1?pq-origsite=gscholar&cbl=18750},
  urldate = {2024-03-04}
}
